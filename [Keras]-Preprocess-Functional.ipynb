{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caef5fb6",
   "metadata": {},
   "source": [
    "# Purpose of this notebook:\n",
    "\n",
    "This notebook recreates the functionality from 'speech_commands' example from the [Tensorflow](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/speech_commands) library to make dataloading and training for keyword detection on microcontrollers more flexible. (At least in my opinion) it provides easie access points to do data augmentation, transfer learning and experiment with different model architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d44143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "from tensorflow.lite.experimental.microfrontend.python.ops import audio_microfrontend_op as frontend_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bb6f1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import audiomentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55ffed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_meta = dict(\n",
    "    wanted_words = ['licht', 'party', 'aus'],\n",
    "    data_path = '/home/average-joe/coding_data/keyword_detection_nano/dataset/',\n",
    "    epochs = 5,\n",
    "    learning_rate = 1e-3,\n",
    "    batch_size = 32,\n",
    ")\n",
    "\n",
    "audio_meta = dict(\n",
    "    sample_rate = 16000,\n",
    "    clip_duration = 1000,\n",
    "    window_size_ms = 30,\n",
    "    window_stride = 20,\n",
    "    feature_bin_count = 40,\n",
    ")\n",
    "\n",
    "desired_samples = int(audio_meta['sample_rate'] * audio_meta['clip_duration'] / 1000)\n",
    "window_size_samples = int(audio_meta['sample_rate'] * audio_meta['window_size_ms'] / 1000)\n",
    "window_stride_samples = int(audio_meta['sample_rate'] * audio_meta['window_stride'] / 1000)\n",
    "length_minus_window = desired_samples - window_size_samples\n",
    "spectrogram_lenght = 1 + int(length_minus_window / window_stride_samples)\n",
    "\n",
    "audio_meta['desired_samples'] = desired_samples\n",
    "audio_meta['spectrogram_lenght'] = spectrogram_lenght\n",
    "audio_meta['fingerprint_size'] = spectrogram_lenght * audio_meta['feature_bin_count']\n",
    "\n",
    "augmentation_meta = dict(\n",
    "    background_frequency = 0.8,\n",
    "    background_volume_range = 0.1,\n",
    "    time_shift_ms = 100.0,\n",
    "    silence_percentage = 0.2,\n",
    "    unknown_percentage = 0.2,\n",
    ")\n",
    "\n",
    "meta_dict = dict(\n",
    "    audio = audio_meta,\n",
    "    augmentation = augmentation_meta,\n",
    "    training = training_meta\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "020567e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeywordDataset(tf.keras.utils.Sequence):\n",
    "    def __init__(self,\n",
    "                 fns,\n",
    "                 background_fns,\n",
    "                 meta_dict,\n",
    "                 batch_size,\n",
    "                 is_validation = False\n",
    "                ):\n",
    "        self.batch_size = batch_size\n",
    "        self.words = meta_dict['training']['wanted_words']\n",
    "        self.vocab = {word: i for i,word in enumerate(['silence', 'unknown'] + self.words)}\n",
    "        self.unknown_fns = fns[1]\n",
    "        self.audio_meta = meta_dict['audio']\n",
    "        self.augmentation_meta = meta_dict['augmentation']\n",
    "        self.is_validation = is_validation\n",
    "        self.augment = audiomentations.Compose([\n",
    "            #audiomentations.PitchShift(min_semitones=2, max_semitones=2, p=1.),\n",
    "            audiomentations.BandPassFilter(p = .5),\n",
    "            audiomentations.HighPassFilter(p = .5),\n",
    "            audiomentations.LowPassFilter(p = .5),\n",
    "            audiomentations.RoomSimulator(leave_length_unchanged = True, p = .5),\n",
    "            audiomentations.TanhDistortion(p = .5)\n",
    "        ])\n",
    "\n",
    "        self.items = self.prepare_items(fns[0])\n",
    "        self.background_data = self.prepare_background_data(background_fns)\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.items) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        items = self.items[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
    "        xs, ys = [], []\n",
    "        for fn in items:\n",
    "            label = self.get_label(fn)\n",
    "            audio = self.get_audio(fn, label).numpy().flatten()\n",
    "            if not self.is_validation:\n",
    "                audio = self.augment(audio, sample_rate = self.audio_meta['sample_rate'])\n",
    "            spectro = self.get_spectrogram(audio)       \n",
    "            xs.append(spectro)\n",
    "            ys.append(self.vocab[label])\n",
    "        return np.stack(xs), np.stack(ys)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if not self.is_validation:\n",
    "            random.shuffle(self.items)\n",
    "        \n",
    "    def prepare_items(self, items):\n",
    "        \"\"\" \n",
    "        Add the same amoung of placeholders for silence as there are unknowns.\n",
    "        Return a shuffled list of items.\n",
    "        To-Do: move this to the filename retrieval\n",
    "        \"\"\"\n",
    "        items = items + ['silence_placeholder'] * len(self.unknown_fns) + self.unknown_fns\n",
    "        random.shuffle(items)\n",
    "        return items\n",
    "\n",
    "    def prepare_background_data(self,fns):\n",
    "        background_data = []\n",
    "        for fn in fns:\n",
    "            file = tf.io.read_file(fn)\n",
    "            audio, _ = tf.audio.decode_wav(file, desired_channels=1)\n",
    "            if len(audio) < self.audio_meta['desired_samples']:\n",
    "                continue\n",
    "            background_data.append(audio)\n",
    "        return background_data\n",
    "        \n",
    "    def get_label(self, fn):\n",
    "        if fn == 'silence_placeholder':\n",
    "            return 'silence'\n",
    "        else:\n",
    "            folder = fn.split('/')[-2]\n",
    "            if folder in self.words:\n",
    "                return folder\n",
    "            return 'unknown'\n",
    "    \n",
    "    def load_audio(self, fn):\n",
    "        file = tf.io.read_file(fn)\n",
    "        audio, _ = tf.audio.decode_wav(contents = file, desired_channels = 1, desired_samples = self.audio_meta['desired_samples'])     \n",
    "        return audio\n",
    "    \n",
    "    def get_timeshift_params(self):\n",
    "        time_shift = self.augmentation_meta['time_shift_ms']\n",
    "        background_frequency = self.augmentation_meta['background_frequency']\n",
    "        background_volume_range = self.augmentation_meta['background_volume_range']\n",
    "        \n",
    "        time_shift_amount = np.random.randint(-time_shift, time_shift) if time_shift > 0 else 0\n",
    "        if time_shift_amount > 0:\n",
    "            time_shift_padding = [[time_shift_amount, 0], [0,0]]\n",
    "            time_shift_offset = [0,0]\n",
    "        else:\n",
    "            time_shift_padding = [[0,-time_shift_amount], [0,0]]\n",
    "            time_shift_offset = [-time_shift_amount, 0]\n",
    "            \n",
    "        return time_shift_padding, time_shift_offset\n",
    "    \n",
    "    def get_random_background(self, label):\n",
    "        background_sample = random.choice(self.background_data)\n",
    "\n",
    "        background_offset = np.random.randint(0, len(background_sample) - self.audio_meta['desired_samples'])\n",
    "        background_clipped = background_sample[background_offset:(background_offset + self.audio_meta['desired_samples'])]\n",
    "        background_reshaped = tf.reshape(background_clipped, [self.audio_meta['desired_samples'],1])\n",
    "        \n",
    "        if label == 'silence':\n",
    "            background_volume = np.random.uniform(0,1)\n",
    "        elif np.random.uniform(0,1) < self.augmentation_meta['background_frequency']:\n",
    "            background_volume = np.random.uniform(0, self.augmentation_meta['background_volume_range'])\n",
    "        else:\n",
    "            background_volume = 0\n",
    "\n",
    "        background_mul = tf.multiply(background_reshaped, background_volume)\n",
    "        return background_mul\n",
    "    \n",
    "    def get_audio(self, fn, label):\n",
    "        if self.is_validation and label != 'silence':\n",
    "            return self.load_audio(fn)\n",
    "        background_mul = self.get_random_background(label)\n",
    "        if label == 'silence':\n",
    "            return background_mul\n",
    "\n",
    "        foreground = self.load_audio(fn)\n",
    "        time_shift_padding, time_shift_offset = self.get_timeshift_params()\n",
    "        \n",
    "        padded_foreground = tf.pad(tensor = foreground, paddings = time_shift_padding, mode = 'CONSTANT')\n",
    "        sliced_foreground = tf.slice(padded_foreground, time_shift_offset, [self.audio_meta['desired_samples'], -1])\n",
    "        background_add = tf.add(background_mul, sliced_foreground)\n",
    "        background_clamp = tf.clip_by_value(background_add, -1., 1.)\n",
    "        \n",
    "        return background_clamp\n",
    "        \n",
    "    def get_spectrogram(self, audio):\n",
    "        int_16_input = tf.cast(tf.multiply(audio, 32768), tf.int16)\n",
    "        micro_frontend = frontend_op.audio_microfrontend(\n",
    "            int_16_input,\n",
    "            sample_rate = self.audio_meta['sample_rate'],\n",
    "            window_size = self.audio_meta['window_size_ms'],\n",
    "            window_step = self.audio_meta['window_stride'],\n",
    "            num_channels = self.audio_meta['feature_bin_count'],\n",
    "            out_scale = 1,\n",
    "            out_type = tf.float32\n",
    "        )\n",
    "        spectro = tf.multiply(micro_frontend, (10. / 256.)).numpy().flatten()\n",
    "        return spectro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93706680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fns(path, wanted_words, val_pct = 0.2, unknown_pct = 0.2, seed = None):\n",
    "    wanted_words_fns = {}\n",
    "    unknown_words_fns = []\n",
    "    background_fns = []\n",
    "    \n",
    "    \"\"\" Get all .wav files contained at the provided path and add them to the appropriate list \"\"\"\n",
    "    fns = glob.glob(os.path.join(path,'*','*.wav'))\n",
    "    for fn in fns:\n",
    "        folder = os.path.split(os.path.dirname(fn))[-1]\n",
    "        if folder == '_background_noise_':\n",
    "            background_fns.append(fn)\n",
    "        elif folder in wanted_words:\n",
    "                if wanted_words_fns.get(folder, False):\n",
    "                    wanted_words_fns[folder].append(fn)\n",
    "                else:\n",
    "                    wanted_words_fns[folder] = [fn]\n",
    "        else:\n",
    "            unknown_words_fns.append(fn)\n",
    "            \n",
    "    \"\"\" Split wanted/unknown in training and validation \"\"\"\n",
    "    training_words = []\n",
    "    validation_words = []\n",
    "    for key in wanted_words_fns.keys():\n",
    "        word_fns = wanted_words_fns[key]\n",
    "        random.shuffle(word_fns)\n",
    "        n_val_word = int(len(word_fns) * val_pct)\n",
    "        validation_words.extend(word_fns[:n_val_word])\n",
    "        training_words.extend(word_fns[n_val_word:])\n",
    "    \n",
    "    n_val_unknown = int(len(unknown_words_fns) * val_pct)\n",
    "    validation_unknowns = unknown_words_fns[:n_val_unknown]\n",
    "    training_unknowns = unknown_words_fns[n_val_unknown:]\n",
    "    \n",
    "    n_training = len(training_words)\n",
    "    n_training_unknown = int(n_training * unknown_pct)\n",
    "    training_unknowns = random.sample(training_unknowns, k = n_training_unknown)\n",
    "    \n",
    "    n_validation = len(validation_words)\n",
    "    n_validation_unknown = int(n_validation * unknown_pct)\n",
    "    validation_unknowns = random.sample(validation_unknowns, k = n_validation_unknown)\n",
    "    \n",
    "    training_fns = [training_words, training_unknowns]\n",
    "    validation_fns = [validation_words, validation_unknowns]\n",
    "    return training_fns, validation_fns, background_fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec1f62e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_fns, validation_fns, background_fns = get_fns(training_meta['data_path'], training_meta['wanted_words'])\n",
    "\n",
    "training_ds = KeywordDataset(\n",
    "    training_fns,\n",
    "    background_fns,\n",
    "    meta_dict,\n",
    "    training_meta['batch_size'],\n",
    "    is_validation = False\n",
    ")\n",
    "validation_ds = KeywordDataset(\n",
    "    validation_fns,\n",
    "    background_fns,\n",
    "    meta_dict,\n",
    "    training_meta['batch_size'] * 2, ## a bigger batch size is possible since no gradients are used\n",
    "    is_validation = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee0775e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sequential API: doesn't work\n",
    "#model = tf.keras.Sequential([\n",
    "#    tf.keras.layers.Reshape((spectrogram_lenght,feature_bin_count,1), input_shape = (fingerprint_size, )),\n",
    "#    tf.keras.layers.Conv2D(filters = 8, kernel_size = (8,10), strides = (2,2), padding = 'same', activation=\"relu\"),\n",
    "#    ##tf.keras.layers.Dropout(0.5),\n",
    "#    ##tf.keras.layers.Flatten(),\n",
    "#    tf.keras.layers.Reshape((4000,)),\n",
    "#    tf.keras.layers.Dense(5, activation = \"softmax\"),\n",
    "#])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c30798fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functional API: does work!\n",
    "fingerprint_size = audio_meta['fingerprint_size']\n",
    "spectrogram_length = audio_meta['spectrogram_lenght']\n",
    "feature_bin_count = audio_meta['feature_bin_count']\n",
    "\n",
    "n_labels = len(training_ds.vocab)\n",
    "\n",
    "inputs = tf.keras.Input(shape = (fingerprint_size,))\n",
    "x = tf.keras.layers.Reshape(target_shape = [-1, spectrogram_length, feature_bin_count, 1])(inputs)\n",
    "x = tf.keras.layers.Conv2D(filters = 8, \n",
    "                           kernel_size = (8, 10), \n",
    "                           strides = (2, 2), \n",
    "                           padding = 'same', \n",
    "                           activation = 'relu')(x)\n",
    "x = tf.keras.layers.ReLU()(x)\n",
    "x = tf.keras.layers.Dropout(0.7)(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "out = tf.keras.layers.Dense(n_labels, \n",
    "                            activation = 'softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs = inputs, outputs = out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f9827a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 1960)]            0         \n",
      "                                                                 \n",
      " reshape_3 (Reshape)         (None, 1, 49, 40, 1)      0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 1, 25, 20, 8)      648       \n",
      "                                                                 \n",
      " re_lu_3 (ReLU)              (None, 1, 25, 20, 8)      0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1, 25, 20, 8)      0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 4000)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 20005     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,653\n",
      "Trainable params: 20,653\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7cc301ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate = training_meta['learning_rate']),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2c600275",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "28/28 [==============================] - 132s 5s/step - loss: 4.1935 - accuracy: 0.2520 - val_loss: 1.3330 - val_accuracy: 0.4695\n",
      "Epoch 2/25\n",
      "28/28 [==============================] - 137s 5s/step - loss: 1.6268 - accuracy: 0.3268 - val_loss: 1.2281 - val_accuracy: 0.5775\n",
      "Epoch 3/25\n",
      "28/28 [==============================] - 136s 5s/step - loss: 1.4666 - accuracy: 0.3590 - val_loss: 1.1055 - val_accuracy: 0.6620\n",
      "Epoch 4/25\n",
      "28/28 [==============================] - 136s 5s/step - loss: 1.4290 - accuracy: 0.4062 - val_loss: 1.0162 - val_accuracy: 0.6948\n",
      "Epoch 5/25\n",
      "28/28 [==============================] - 137s 5s/step - loss: 1.3402 - accuracy: 0.4545 - val_loss: 0.9105 - val_accuracy: 0.7136\n",
      "Epoch 6/25\n",
      "28/28 [==============================] - 133s 5s/step - loss: 1.3252 - accuracy: 0.4672 - val_loss: 0.8645 - val_accuracy: 0.7793\n",
      "Epoch 7/25\n",
      "28/28 [==============================] - 128s 5s/step - loss: 1.2614 - accuracy: 0.4948 - val_loss: 0.7332 - val_accuracy: 0.7887\n",
      "Epoch 8/25\n",
      "28/28 [==============================] - 124s 4s/step - loss: 1.1522 - accuracy: 0.5639 - val_loss: 0.6671 - val_accuracy: 0.8169\n",
      "Epoch 9/25\n",
      "28/28 [==============================] - 130s 5s/step - loss: 1.1629 - accuracy: 0.5593 - val_loss: 0.6437 - val_accuracy: 0.8122\n",
      "Epoch 10/25\n",
      "28/28 [==============================] - 129s 5s/step - loss: 1.1648 - accuracy: 0.5478 - val_loss: 0.6342 - val_accuracy: 0.8216\n",
      "Epoch 11/25\n",
      "28/28 [==============================] - 128s 5s/step - loss: 1.0788 - accuracy: 0.5857 - val_loss: 0.5755 - val_accuracy: 0.7981\n",
      "Epoch 12/25\n",
      "28/28 [==============================] - 129s 5s/step - loss: 0.9865 - accuracy: 0.6226 - val_loss: 0.5974 - val_accuracy: 0.8122\n",
      "Epoch 13/25\n",
      "28/28 [==============================] - 128s 5s/step - loss: 1.0992 - accuracy: 0.5915 - val_loss: 0.5776 - val_accuracy: 0.8263\n",
      "Epoch 14/25\n",
      "28/28 [==============================] - 129s 5s/step - loss: 1.0080 - accuracy: 0.6191 - val_loss: 0.5187 - val_accuracy: 0.8451\n",
      "Epoch 15/25\n",
      "28/28 [==============================] - 129s 5s/step - loss: 1.0716 - accuracy: 0.5972 - val_loss: 0.5898 - val_accuracy: 0.8169\n",
      "Epoch 16/25\n",
      "28/28 [==============================] - 126s 4s/step - loss: 1.0586 - accuracy: 0.6064 - val_loss: 0.5182 - val_accuracy: 0.8451\n",
      "Epoch 17/25\n",
      "28/28 [==============================] - 131s 5s/step - loss: 1.0026 - accuracy: 0.6168 - val_loss: 0.5104 - val_accuracy: 0.8263\n",
      "Epoch 18/25\n",
      "28/28 [==============================] - 131s 5s/step - loss: 1.0272 - accuracy: 0.5995 - val_loss: 0.4914 - val_accuracy: 0.8404\n",
      "Epoch 19/25\n",
      "28/28 [==============================] - 128s 5s/step - loss: 1.0049 - accuracy: 0.6180 - val_loss: 0.4960 - val_accuracy: 0.8592\n",
      "Epoch 20/25\n",
      "28/28 [==============================] - 125s 4s/step - loss: 0.9684 - accuracy: 0.6467 - val_loss: 0.5139 - val_accuracy: 0.8310\n",
      "Epoch 21/25\n",
      "28/28 [==============================] - 124s 4s/step - loss: 1.0120 - accuracy: 0.6237 - val_loss: 0.4746 - val_accuracy: 0.8498\n",
      "Epoch 22/25\n",
      "28/28 [==============================] - 128s 5s/step - loss: 0.9468 - accuracy: 0.6559 - val_loss: 0.5055 - val_accuracy: 0.8404\n",
      "Epoch 23/25\n",
      "28/28 [==============================] - 126s 4s/step - loss: 0.9686 - accuracy: 0.6421 - val_loss: 0.5377 - val_accuracy: 0.8310\n",
      "Epoch 24/25\n",
      "28/28 [==============================] - 132s 5s/step - loss: 0.8829 - accuracy: 0.6709 - val_loss: 0.5228 - val_accuracy: 0.8310\n",
      "Epoch 25/25\n",
      "28/28 [==============================] - 135s 5s/step - loss: 0.9490 - accuracy: 0.6502 - val_loss: 0.5117 - val_accuracy: 0.8592\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    training_ds,\n",
    "    validation_data = validation_ds,\n",
    "    epochs = 25,\n",
    "    verbose = 1,\n",
    "    shuffle = False, ## is handled by dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7af475e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/functional_augmented_859/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/functional_augmented_859/assets\n"
     ]
    }
   ],
   "source": [
    "export_name = 'functional_augmented_859'\n",
    "export_dir = f'saved_model/{export_name}'\n",
    "tf.saved_model.save(model, export_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d0724b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "REP_DATA_SIZE = 100\n",
    "def representative_dataset_gen():\n",
    "    for i in range(REP_DATA_SIZE):\n",
    "            fn = random.choice(validation_ds.items)\n",
    "            label = validation_ds.get_label(fn)\n",
    "            audio = validation_ds.get_audio(fn, label)\n",
    "            spectro = validation_ds.get_spectrogram(audio).reshape(1,1960)\n",
    "            \n",
    "            yield [spectro]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d94078a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized modelsize: 23752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-09 16:33:33.074444: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-08-09 16:33:33.074472: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-08-09 16:33:33.075026: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: saved_model/functional_augmented_859\n",
      "2022-08-09 16:33:33.076320: I tensorflow/cc/saved_model/reader.cc:81] Reading meta graph with tags { serve }\n",
      "2022-08-09 16:33:33.076334: I tensorflow/cc/saved_model/reader.cc:122] Reading SavedModel debug info (if present) from: saved_model/functional_augmented_859\n",
      "2022-08-09 16:33:33.079187: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2022-08-09 16:33:33.079990: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-08-09 16:33:33.109710: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: saved_model/functional_augmented_859\n",
      "2022-08-09 16:33:33.120511: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 45507 microseconds.\n",
      "2022-08-09 16:33:33.142030: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: 9, output_inference_type: 9\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.inference_input_type = tf.compat.v1.lite.constants.INT8 \n",
    "converter.inference_output_type = tf.compat.v1.lite.constants.INT8\n",
    "\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "tflite_model = converter.convert()\n",
    "tflite_model_size = open(f\"models/{export_name}.tflite\", \"wb\").write(tflite_model)\n",
    "print(f\"Quantized modelsize: {tflite_model_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7499062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!xxd -i models/{export_name}.tflite > models/{export_name}.cc\n",
    "#REPLACE_TEXT = MODEL_TFLITE.replace('/', '_').replace('.', '_')\n",
    "#!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {MODEL_TFLITE_MICRO}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1bb42f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!code models/{export_name}.cc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
