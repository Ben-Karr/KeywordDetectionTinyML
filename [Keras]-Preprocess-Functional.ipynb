{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06c8bd3f",
   "metadata": {},
   "source": [
    "# Keyword Detection for Microcontrollers with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bf4c0f",
   "metadata": {},
   "source": [
    "It's expected that all words are collected in a single folder and each sound file is given as .wav, clipped to 1ms. All files should be contained in the folder that matches their label and that the folder name for background noise is `_background_noise_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d44143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from pprint import pprint\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "from tensorflow.lite.experimental.microfrontend.python.ops import audio_microfrontend_op as frontend_op\n",
    "\n",
    "import audiomentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1686a426",
   "metadata": {},
   "source": [
    "# Training Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80254f18",
   "metadata": {},
   "source": [
    "We use a dictionary as a single place to hold metadata concerning training, audio settings and augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ffed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta(meta_dict={}, **kwargs):\n",
    "    \"\"\" \n",
    "        Retrieve the parameters from the provided dict or, if it doesn't exist, use a default value.\n",
    "        If you want to pass just a few custom parameters you can use kwarg arguments otherwise you can pass \n",
    "        them as a dict. Function throws an error if the same keyword is passed more than once to make sure that the \n",
    "        intended value is used. Defaul values are from https://colab.research.google.com/github/tinyMLx/colabs/blob/master/4-6-8-CustomDatasetKWSModel.ipynb\n",
    "    \"\"\"\n",
    "    merged_meta =  {**meta_dict, **kwargs}\n",
    "    assert len(merged_meta) == len(meta_dict) + len(kwargs), \"It appears that a key was set more than once.\"\n",
    "    \n",
    "    training, audio, augments = {}, {}, {}\n",
    "    training['wanted_words']            = merged_meta.get('wanted_words', ['on', 'off'])\n",
    "    training['data_path']               = merged_meta.get('data_path', 'dataset/')\n",
    "    training['epochs']                  = merged_meta.get('epochs', 5)\n",
    "    training['learning_rate']           = merged_meta.get('learning_rate', 1e-3)\n",
    "    training['batch_size']              = merged_meta.get('batch_size', 32)\n",
    "    training['excluded_words']          = merged_meta.get('excluded_words', [])\n",
    "\n",
    "    audio['sample_rate']                = merged_meta.get('sample_rate', 16_000)\n",
    "    audio['clip_duration']              = merged_meta.get('clip_duration', 1000)\n",
    "    audio['window_size_ms']             = merged_meta.get('window_size_ms', 30)\n",
    "    audio['window_stride']              = merged_meta.get('window_stride', 20)\n",
    "    audio['feature_bin_count']          = merged_meta.get('feature_bin_count', 40)\n",
    "    audio['desired_samples']            = int(audio['sample_rate'] * audio['clip_duration'] / 1000)\n",
    "    window_size_samples                 = int(audio['sample_rate'] * audio['window_size_ms'] / 1000)\n",
    "    window_stride_samples               = int(audio['sample_rate'] * audio['window_stride'] / 1000)\n",
    "    length_minus_window                 = audio['desired_samples'] - window_size_samples\n",
    "    spectrogram_lenght                  = 1 + int(length_minus_window / window_stride_samples)\n",
    "    audio['spectrogram_lenght']         = spectrogram_lenght\n",
    "    audio['fingerprint_size']           = spectrogram_lenght * audio['feature_bin_count']\n",
    "    \n",
    "    augments['background_frequency']    = merged_meta.get('background_frequency', 0.8)\n",
    "    augments['background_volume_range'] = merged_meta.get('background_volume_range', 0.1)\n",
    "    augments['time_shift_ms']           = merged_meta.get('time_shift_ms', 100.0)\n",
    "    augments['silence_percentage']      = merged_meta.get('silence_percentage', 0.2)\n",
    "    augments['unknown_percentage']      = merged_meta.get('unknown_percentage', 0.2)\n",
    "        \n",
    "    return dict(training=training, audio=audio, augmentation=augments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5363989",
   "metadata": {},
   "source": [
    "If you want to do transfer learning you can use `get_pretrain_words` to pull some random words to pretrain on. Exclude the words you want to fine tune for later and the function can return a random set of words to use for training while leaving a few as `unknown` words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b53edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrain_words(path, excluded_words, shuffle=False, n = 25):\n",
    "    \"\"\"\n",
    "    Pulls all folders/words found at `path`. Considers only those that are not in the `excluded_words` and returns \n",
    "    `n` of those.\n",
    "    \"\"\"\n",
    "    all_folders = [folder.split('/')[-1] for folder in glob.glob(path+'*')]\n",
    "    all_included_words = [folder for folder in all_folders if folder not in (excluded_words + ['_background_noise_'])]\n",
    "    if shuffle:\n",
    "        random.shuffle(all_included_words)\n",
    "    return all_included_words[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4cfe10",
   "metadata": {},
   "source": [
    "## Build dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69a97d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_words = ['licht', 'aus', 'party']\n",
    "data_path = os.path.join(os.path.expanduser(\"~\"),'coding_data/keyword_detection_nano/dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064c88ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For pretraining\n",
    "meta_dict = get_meta(\n",
    "    data_path = data_path,\n",
    "    wanted_words = get_pretrain_words(data_path, excluded_words),\n",
    "    excluded_words = ['yes', 'no', 'left'],\n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aa15d3",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "Most of the functionality in this class comes from the [speech_commands](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/speech_commands) example in the tensorflow library with a few simplifications and additions for convenience. In particular the variable names are kept the same where possible so you can quickly find the coresponding one in the original files. This class only implements the case for `preprocessing=='micro'` since it's aimed to do inference on microcontrollers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020567e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeywordDataset(tf.keras.utils.Sequence):\n",
    "    \"Test\"\n",
    "    def __init__(self,\n",
    "                 fns,\n",
    "                 background_fns,\n",
    "                 meta_dict,\n",
    "                 batch_size,\n",
    "                 is_validation = False\n",
    "                ):\n",
    "        self.items = fns\n",
    "        self.words = meta_dict['training']['wanted_words']\n",
    "        self.vocab = {word: i for i,word in enumerate(['silence', 'unknown'] + self.words)}\n",
    "        self.audio_meta = meta_dict['audio']\n",
    "        self.augmentation_meta = meta_dict['augmentation']\n",
    "        self.is_validation = is_validation\n",
    "        self.augment = self.build_augments()\n",
    "        self.background_data = self.prepare_background_data(background_fns)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.items) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Pulls a subset of filenames of size `batch_size`. Loads the audio file according to its label and adds \n",
    "        augmentations if in 'training mode'. Finally creates a spectrogram and combines the batch to single \n",
    "        numpy x,y vectors.\n",
    "        \"\"\"\n",
    "        items = self.items[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
    "        xs, ys = [], []\n",
    "        for fn in items:\n",
    "            label = self.get_label(fn)\n",
    "            audio = self.get_audio(fn, label).numpy().flatten()\n",
    "            if not self.is_validation:\n",
    "                audio = self.augment(audio, sample_rate = self.audio_meta['sample_rate'])\n",
    "            spectro = self.get_spectrogram(audio)       \n",
    "            xs.append(spectro)\n",
    "            ys.append(self.vocab[label])\n",
    "        return np.stack(xs), np.stack(ys)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if not self.is_validation:\n",
    "            random.shuffle(self.items)\n",
    "\n",
    "    def prepare_background_data(self,fns):\n",
    "        ## See `prepare_background_data` in tensorflow/examples/speech_commands/input_data.py\n",
    "        background_data = []\n",
    "        for fn in fns:\n",
    "            file = tf.io.read_file(fn)\n",
    "            audio, _ = tf.audio.decode_wav(file, desired_channels=1)\n",
    "            if len(audio) < self.audio_meta['desired_samples']:\n",
    "                continue\n",
    "            background_data.append(audio)\n",
    "        return background_data\n",
    "        \n",
    "    def get_label(self, fn):\n",
    "        if fn == 'silence_placeholder':\n",
    "            return 'silence'\n",
    "        else:\n",
    "            folder = fn.split('/')[-2]\n",
    "            if folder in self.words:\n",
    "                return folder\n",
    "            return 'unknown'\n",
    "    \n",
    "    def load_audio(self, fn):\n",
    "        file = tf.io.read_file(fn)\n",
    "        audio, _ = tf.audio.decode_wav(contents = file, \n",
    "                                       desired_channels = 1, \n",
    "                                       desired_samples = self.audio_meta['desired_samples']\n",
    "                                      )     \n",
    "        return audio\n",
    "    \n",
    "    def get_timeshift_params(self):\n",
    "        ## See `get_data` in tensorflow/examples/speech_commands/input_data.py\n",
    "        time_shift = self.augmentation_meta['time_shift_ms']\n",
    "        background_frequency = self.augmentation_meta['background_frequency']\n",
    "        background_volume_range = self.augmentation_meta['background_volume_range']\n",
    "        \n",
    "        time_shift_amount = np.random.randint(-time_shift, time_shift) if time_shift > 0 else 0\n",
    "        if time_shift_amount > 0:\n",
    "            time_shift_padding = [[time_shift_amount, 0], [0,0]]\n",
    "            time_shift_offset = [0,0]\n",
    "        else:\n",
    "            time_shift_padding = [[0,-time_shift_amount], [0,0]]\n",
    "            time_shift_offset = [-time_shift_amount, 0]\n",
    "            \n",
    "        return time_shift_padding, time_shift_offset\n",
    "    \n",
    "    def get_random_background(self, label):\n",
    "        ## See `get_data` in tensorflow/examples/speech_commands/input_data.py\n",
    "        background_sample = random.choice(self.background_data)\n",
    "\n",
    "        background_offset = np.random.randint(0, len(background_sample) - self.audio_meta['desired_samples'])\n",
    "        background_clipped = background_sample[background_offset:(background_offset + self.audio_meta['desired_samples'])]\n",
    "        background_reshaped = tf.reshape(background_clipped, [self.audio_meta['desired_samples'],1])\n",
    "        \n",
    "        if label == 'silence':\n",
    "            background_volume = np.random.uniform(0,1)\n",
    "        elif np.random.uniform(0,1) < self.augmentation_meta['background_frequency']:\n",
    "            background_volume = np.random.uniform(0, self.augmentation_meta['background_volume_range'])\n",
    "        else:\n",
    "            background_volume = 0\n",
    "\n",
    "        background_mul = tf.multiply(background_reshaped, background_volume)\n",
    "        return background_mul\n",
    "    \n",
    "    def get_audio(self, fn, label):\n",
    "        \"\"\"\n",
    "        Adds random background to audio and shifts it a bit back or forth if in 'training mode', \n",
    "        returns background-only if label is `silence`.\n",
    "        \"\"\"\n",
    "        if self.is_validation and label != 'silence':\n",
    "            return self.load_audio(fn)\n",
    "        background_mul = self.get_random_background(label)\n",
    "        if label == 'silence':\n",
    "            return background_mul\n",
    "        \n",
    "        ## See `prepare_processing_graph` in tensorflow/examples/speech_commands/input_data.py\n",
    "        foreground = self.load_audio(fn)\n",
    "        time_shift_padding, time_shift_offset = self.get_timeshift_params()\n",
    "        \n",
    "        padded_foreground = tf.pad(tensor = foreground, paddings = time_shift_padding, mode = 'CONSTANT')\n",
    "        sliced_foreground = tf.slice(padded_foreground, time_shift_offset, [self.audio_meta['desired_samples'], -1])\n",
    "        background_add = tf.add(background_mul, sliced_foreground)\n",
    "        background_clamp = tf.clip_by_value(background_add, -1., 1.)\n",
    "        \n",
    "        return background_clamp\n",
    "        \n",
    "    def get_spectrogram(self, audio):\n",
    "        ## See `prepare_processing_graph` in tensorflow/examples/speech_commands/input_data.py\n",
    "        int_16_input = tf.cast(tf.multiply(audio, 32768), tf.int16)\n",
    "        micro_frontend = frontend_op.audio_microfrontend(\n",
    "            int_16_input,\n",
    "            sample_rate = self.audio_meta['sample_rate'],\n",
    "            window_size = self.audio_meta['window_size_ms'],\n",
    "            window_step = self.audio_meta['window_stride'],\n",
    "            num_channels = self.audio_meta['feature_bin_count'],\n",
    "            out_scale = 1,\n",
    "            out_type = tf.float32\n",
    "        )\n",
    "        flat_spectro = tf.multiply(micro_frontend, (10. / 256.)).numpy().flatten()\n",
    "        return flat_spectro\n",
    "    \n",
    "    def build_augments(self):\n",
    "        ## Uses the audiomentations library. Check https://github.com/iver56/audiomentations for further details.\n",
    "        augs = audiomentations.Compose([\n",
    "            audiomentations.ClippingDistortion(max_percentile_threshold=20, p=.5),\n",
    "            audiomentations.HighPassFilter(min_cutoff_freq=1000, p=.3),\n",
    "            audiomentations.LowPassFilter(min_cutoff_freq=1000, p=.3),\n",
    "            audiomentations.GainTransition(min_gain_in_db=-12,max_gain_in_db=12,min_duration=0.1,max_duration=0.9,duration_unit='fraction',p=.5),\n",
    "            audiomentations.PitchShift(min_semitones=-1, max_semitones=1, p=.3),\n",
    "            audiomentations.SevenBandParametricEQ(p=.5),\n",
    "            audiomentations.PolarityInversion(p=0.3),\n",
    "            audiomentations.TimeMask(p=.3),\n",
    "            audiomentations.AddGaussianNoise(max_amplitude=0.005, p=0.3),\n",
    "        ])\n",
    "        return augs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aaf918",
   "metadata": {},
   "source": [
    "### Retreive files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78603e0",
   "metadata": {},
   "source": [
    "To hit a certain percentage of `unknown` / `silent` labels we use the following calculation:\n",
    "If $p_s$ is the percentage of `silent` and $p_u$ is the percentage of `unknown` labels in the __finished__ dataset than this means \n",
    "$$\n",
    "p_s = \\frac{n_s}{n + n_s + n_u}, \\quad\n",
    "p_u = \\frac{n_u}{n + n_s + n_u}\n",
    "$$ where $n$ is the total number of instances $n_s$ those of label `silence`, $n_u$ those of label `unknown`. Solving the equations for $n_s$ and $n_u$ gives:\n",
    "$$\n",
    "n_s = \\frac{p_s \\cdot (n + \\frac{p_u \\cdot n}{1-p_u})}{1 - p_s - \\frac{p_s \\cdot p_u}{1-p_u}}, \\quad\n",
    "n_u = \\frac{p_u \\cdot (n + n_s)}{1 - p_u}.\n",
    "$$\n",
    "The function below does this calculation and returns the next bigger integers. The rounding is for computational stability (e.g. 20.0000000001 should be 20 not 21)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e80463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_unknown_silent_n(n, p_s, p_u):\n",
    "    n_s = (p_s * (n+ (p_u * n)/(1-p_u))) / (1 - ((p_s * p_u)/(1-p_u)) - p_s)\n",
    "    n_u = (p_u * (n+n_s)/(1-p_u))\n",
    "    return math.ceil(round(n_s, 3)), math.ceil(round(n_u, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93706680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fns(path, wanted_words, excluded_words = [], desired_samples = 16_000, val_pct = 0.2, silent_pct = 0.2, unknown_pct = 0.2, seed = None):\n",
    "    \"\"\"\n",
    "    path:           Where to search for *.wav files\n",
    "    wanted_words:   The keywords that you want to detect with your model\n",
    "    excluded_wods:  Words that should not be used in training. \n",
    "                    Words that are neither wanted nor excluded will be used as unknown_words\n",
    "    val_pct:        Percentage of files that should be used for validation\n",
    "    unknown_pct:    Percentage of the train/val split that are unknown\n",
    "    silent_pct:     Percentage of the train/val split that are labeled silent\n",
    "    \"\"\"\n",
    "    wanted_fns_dict = {}\n",
    "    unknown_fns = []\n",
    "    background_fns = []\n",
    "    ## Get all but excluded .wav files contained at the provided path and add them to the appropriate list\n",
    "    wavs = glob.glob(os.path.join(path,'*','*.wav'))\n",
    "    for fn in wavs:\n",
    "        folder = os.path.split(os.path.dirname(fn))[-1]\n",
    "        try: ## excepts if the audio file has a different number of samples as `desired_samples`\n",
    "            tf.audio.decode_wav(contents = tf.io.read_file(fn), desired_channels = 1, desired_samples = desired_samples)\n",
    "        except:\n",
    "            continue\n",
    "        if folder in excluded_words:\n",
    "            continue\n",
    "        if folder == '_background_noise_':\n",
    "            background_fns.append(fn)\n",
    "        elif folder in wanted_words:\n",
    "            ## Creates a list containing fn at keyword if the keyword isn't contained in the dict yet,\n",
    "            ## else adds fn to list.\n",
    "            if wanted_fns_dict.get(folder, False):\n",
    "                wanted_fns_dict[folder].append(fn)\n",
    "            else:\n",
    "                wanted_fns_dict[folder] = [fn]\n",
    "        else:\n",
    "            unknown_fns.append(fn)\n",
    "            \n",
    "    ## Split wanted/unknown in training and validation, for each wanted words: val_pct of the total number of \n",
    "    ## per word fns are in the validation set (1-val_pct) in the training set.\n",
    "    training_words = []\n",
    "    validation_words = []\n",
    "    for key in wanted_fns_dict.keys():\n",
    "        word_fns = wanted_fns_dict[key]\n",
    "        random.shuffle(word_fns)\n",
    "        n_val_word = int(len(word_fns) * val_pct)\n",
    "        validation_words.extend(word_fns[:n_val_word])\n",
    "        training_words.extend(word_fns[n_val_word:])\n",
    "    \n",
    "    ## Calcs number of silent/unknown for train/val split to hit a certain percentage\n",
    "    n_silent_train, n_unknown_train = calc_unknown_silent_n(len(training_words), silent_pct, unknown_pct)\n",
    "    n_silent_val, n_unknown_val = calc_unknown_silent_n(len(validation_words), silent_pct, unknown_pct)\n",
    "    \n",
    "    ## Keep validation determenistic\n",
    "    validation_unknown = unknown_fns[:n_unknown_train]\n",
    "    non_validation_unknown = unknown_fns[n_unknown_train:]\n",
    "    ## Pick training unknowns at random\n",
    "    random.shuffle(non_validation_unknown)\n",
    "    training_unknown = non_validation_unknown[:n_unknown_train]\n",
    "    \n",
    "    training_fns = training_words + training_unknown + ['silence_placeholder'] * n_silent_train\n",
    "    validation_fns = validation_words + validation_unknown + ['silence_placeholder'] * n_silent_val\n",
    "        \n",
    "    random.shuffle(training_fns)\n",
    "    random.shuffle(validation_fns)\n",
    "    random.shuffle(background_fns)\n",
    "    \n",
    "    return training_fns, validation_fns, background_fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1f62e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-16 10:52:50.684358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-16 10:52:50.711767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-16 10:52:50.711974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-16 10:52:50.730458: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-16 10:52:50.730995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-16 10:52:50.731174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-16 10:52:50.731346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-16 10:52:51.059550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-16 10:52:51.059743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-16 10:52:51.059897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-16 10:52:51.060025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6185 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:26:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "training_fns, validation_fns, background_fns = get_fns(meta_dict['training']['data_path'],\n",
    "                                                       meta_dict['training']['wanted_words'],\n",
    "                                                       desired_samples = meta_dict['audio']['desired_samples']\n",
    "                                                      )\n",
    "training_ds = KeywordDataset(\n",
    "    training_fns,\n",
    "    background_fns,\n",
    "    meta_dict,\n",
    "    meta_dict['training']['batch_size'],\n",
    "    is_validation = False\n",
    ")\n",
    "validation_ds = KeywordDataset(\n",
    "    validation_fns,\n",
    "    background_fns,\n",
    "    meta_dict,\n",
    "    meta_dict['training']['batch_size'] * 2, ## a bigger batch size is possible since no gradients are used\n",
    "    is_validation = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aea044",
   "metadata": {},
   "source": [
    "# Build model (from pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0775e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## # Sequential API: doesn't work\n",
    "## model = tf.keras.Sequential([\n",
    "##     tf.keras.layers.Reshape((spectrogram_lenght,feature_bin_count,1), input_shape = (fingerprint_size, )),\n",
    "##     tf.keras.layers.Conv2D(filters = 8, kernel_size = (8,10), strides = (2,2), padding = 'same', activation=\"relu\"),\n",
    "##     ## tf.keras.layers.Dropout(0.5),\n",
    "##     ## tf.keras.layers.Flatten(),\n",
    "##     tf.keras.layers.Reshape((4000,)),\n",
    "##     tf.keras.layers.Dense(5, activation = \"softmax\"),\n",
    "## ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61f7c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functional API: does work!\n",
    "def create_model(arch, n_labels, meta_dict, dropout = 0.5):\n",
    "    \"\"\"\n",
    "    arch: One of `tiny_conv`, `tiny_embedding_conv`, `small_cnov`\n",
    "    n_labels: Number of output nodes, corresponding to the number of labels\n",
    "    \n",
    "    Implementation of `tiny_conv` and `tiny_embedding_conv` copies tensorflow/speech_commands/models.py,\n",
    "    \"\"\"\n",
    "    fingerprint_size = meta_dict['audio']['fingerprint_size']\n",
    "    spectrogram_length = meta_dict['audio']['spectrogram_lenght']\n",
    "    feature_bin_count = meta_dict['audio']['feature_bin_count']\n",
    "    \n",
    "    inputs = tf.keras.Input(shape = (fingerprint_size,))\n",
    "    x = tf.keras.layers.Reshape(target_shape = [-1, spectrogram_length, feature_bin_count, 1])(inputs)\n",
    "    if arch == 'tiny_conv':\n",
    "        ## Returns the same model as create_tiny_conv_model in tensorflow/speech_commands/models.py\n",
    "        x = tf.keras.layers.Conv2D(filters = 8, \n",
    "                                   kernel_size = (8, 10), \n",
    "                                   strides = (2, 2), \n",
    "                                   padding = 'same', \n",
    "                                   activation = 'relu')(x)\n",
    "        x = tf.keras.layers.Dropout(dropout)(x)\n",
    "                \n",
    "    if arch == 'tiny_embedding_conv':\n",
    "        ## Returns the same model as create_tiny_embedding_conv_model in tensorflow/speech_commands/models.py\n",
    "        x = tf.keras.layers.Conv2D(filters = 8,\n",
    "                                  kernel_size = (8,10),\n",
    "                                  strides = (2, 2),\n",
    "                                  padding = 'same',\n",
    "                                  activation = 'relu')(x)\n",
    "        x = tf.keras.layers.Dropout(dropout)(x)\n",
    "        x = tf.keras.layers.Conv2D(filters = 8,\n",
    "                                  kernel_size = (8,10),\n",
    "                                  strides = (8,8),\n",
    "                                  padding = 'same',\n",
    "                                  activation = 'relu')(x)\n",
    "        x = tf.keras.layers.Dropout(dropout)(x)\n",
    "        \n",
    "    if arch == 'small_conv':\n",
    "        ## Add a same-size convolution then downsample\n",
    "        x = tf.keras.layers.Conv2D(filters = 16,\n",
    "                                   kernel_size = (3,5),\n",
    "                                   strides = (1,1),\n",
    "                                   padding = 'same',\n",
    "                                   activation = 'relu')(x)\n",
    "        x = tf.keras.layers.Dropout(dropout)(x)\n",
    "        x = tf.keras.layers.Conv2D(filters = 8, \n",
    "                                   kernel_size = (8, 10), \n",
    "                                   strides = (2, 2), \n",
    "                                   padding = 'same', \n",
    "                                   activation = 'relu')(x)\n",
    "        x = tf.keras.layers.Dropout(dropout)(x)\n",
    "        \n",
    "    x = tf.keras.layers.Flatten()(x)    \n",
    "    out = tf.keras.layers.Dense(n_labels, activation = 'softmax')(x)\n",
    "    return tf.keras.Model(inputs = inputs, outputs = out)\n",
    "\n",
    "def get_model(n_labels, meta_dict, arch = 'tiny_conv', dropout = 0.5, pretrain_path = False):\n",
    "    ## When loading from pretrained model, remove the last, dense layer and replace by a Dense layer with `n_labels` output nodes\n",
    "    if pretrain_path:\n",
    "        model = tf.keras.models.load_model(pretrain_path)\n",
    "        model.trainable = False ## Freezes all but the classification layers\n",
    "        output = tf.keras.layers.Dense(n_labels, activation = 'softmax')(model.layers[-2].output)\n",
    "        return tf.keras.Model(inputs = model.input, outputs = output)\n",
    "    return create_model(arch, n_labels, meta_dict, dropout = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242ec556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1960)]            0         \n",
      "                                                                 \n",
      " reshape_2 (Reshape)         (None, 1, 49, 40, 1)      0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 1, 49, 40, 16)     256       \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 1, 49, 40, 16)     0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 1, 25, 20, 8)      10248     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 1, 25, 20, 8)      0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 4000)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 27)                108027    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 118,531\n",
      "Trainable params: 118,531\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_labels = len(training_ds.vocab)\n",
    "model = get_model(n_labels, meta_dict, arch='small_conv')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebbc126",
   "metadata": {},
   "source": [
    "# Train model and save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ae1459",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate = meta_dict['training']['learning_rate']),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec1cc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "2876/2876 [==============================] - 581s 202ms/step - loss: 2.3579 - accuracy: 0.3697 - val_loss: 1.8411 - val_accuracy: 0.4948\n",
      "Epoch 2/25\n",
      "2876/2876 [==============================] - 576s 200ms/step - loss: 1.9384 - accuracy: 0.4665 - val_loss: 1.6331 - val_accuracy: 0.5525\n",
      "Epoch 3/25\n",
      "2876/2876 [==============================] - 573s 199ms/step - loss: 1.8186 - accuracy: 0.4947 - val_loss: 1.5418 - val_accuracy: 0.5726\n",
      "Epoch 4/25\n",
      "2876/2876 [==============================] - 570s 198ms/step - loss: 1.7522 - accuracy: 0.5114 - val_loss: 1.5310 - val_accuracy: 0.5779\n",
      "Epoch 5/25\n",
      "2876/2876 [==============================] - 571s 198ms/step - loss: 1.7022 - accuracy: 0.5262 - val_loss: 1.7740 - val_accuracy: 0.5465\n",
      "Epoch 6/25\n",
      "2876/2876 [==============================] - 606s 211ms/step - loss: 1.6645 - accuracy: 0.5371 - val_loss: 1.5952 - val_accuracy: 0.5704\n",
      "Epoch 7/25\n",
      "2876/2876 [==============================] - 577s 201ms/step - loss: 1.6430 - accuracy: 0.5428 - val_loss: 1.7053 - val_accuracy: 0.5484\n",
      "Epoch 8/25\n",
      "2876/2876 [==============================] - 591s 206ms/step - loss: 1.6162 - accuracy: 0.5506 - val_loss: 1.6038 - val_accuracy: 0.5639\n",
      "Epoch 9/25\n",
      "2876/2876 [==============================] - 590s 205ms/step - loss: 1.5976 - accuracy: 0.5554 - val_loss: 1.5874 - val_accuracy: 0.5770\n",
      "Epoch 10/25\n",
      "2876/2876 [==============================] - 592s 206ms/step - loss: 1.5778 - accuracy: 0.5612 - val_loss: 1.5735 - val_accuracy: 0.5734\n",
      "Epoch 11/25\n",
      "2876/2876 [==============================] - 591s 205ms/step - loss: 1.5550 - accuracy: 0.5681 - val_loss: 1.4744 - val_accuracy: 0.6020\n",
      "Epoch 12/25\n",
      "2876/2876 [==============================] - 565s 197ms/step - loss: 1.5516 - accuracy: 0.5698 - val_loss: 1.4186 - val_accuracy: 0.5988\n",
      "Epoch 13/25\n",
      "2876/2876 [==============================] - 573s 199ms/step - loss: 1.5260 - accuracy: 0.5749 - val_loss: 1.4827 - val_accuracy: 0.5794\n",
      "Epoch 14/25\n",
      "2876/2876 [==============================] - 574s 200ms/step - loss: 1.5204 - accuracy: 0.5777 - val_loss: 1.6330 - val_accuracy: 0.5595\n",
      "Epoch 15/25\n",
      "2876/2876 [==============================] - 574s 200ms/step - loss: 1.5220 - accuracy: 0.5765 - val_loss: 1.4746 - val_accuracy: 0.6072\n",
      "Epoch 16/25\n",
      "2876/2876 [==============================] - 570s 198ms/step - loss: 1.5152 - accuracy: 0.5793 - val_loss: 1.6206 - val_accuracy: 0.5905\n",
      "Epoch 17/25\n",
      "2876/2876 [==============================] - 571s 198ms/step - loss: 1.5120 - accuracy: 0.5800 - val_loss: 1.4197 - val_accuracy: 0.6092\n",
      "Epoch 18/25\n",
      "2876/2876 [==============================] - 578s 201ms/step - loss: 1.5020 - accuracy: 0.5824 - val_loss: 1.5590 - val_accuracy: 0.5927\n",
      "Epoch 19/25\n",
      "2876/2876 [==============================] - 579s 201ms/step - loss: 1.4884 - accuracy: 0.5871 - val_loss: 1.6318 - val_accuracy: 0.5877\n",
      "Epoch 20/25\n",
      "2876/2876 [==============================] - 577s 200ms/step - loss: 1.4938 - accuracy: 0.5842 - val_loss: 1.5512 - val_accuracy: 0.5953\n",
      "Epoch 21/25\n",
      "2876/2876 [==============================] - 591s 205ms/step - loss: 1.4925 - accuracy: 0.5865 - val_loss: 1.5920 - val_accuracy: 0.5895\n",
      "Epoch 22/25\n",
      "2876/2876 [==============================] - 593s 206ms/step - loss: 1.4893 - accuracy: 0.5883 - val_loss: 1.5092 - val_accuracy: 0.5884\n",
      "Epoch 23/25\n",
      "2876/2876 [==============================] - 588s 205ms/step - loss: 1.4828 - accuracy: 0.5876 - val_loss: 1.6819 - val_accuracy: 0.5771\n",
      "Epoch 24/25\n",
      "2876/2876 [==============================] - 577s 200ms/step - loss: 1.4774 - accuracy: 0.5902 - val_loss: 1.5797 - val_accuracy: 0.6002\n",
      "Epoch 25/25\n",
      "2876/2876 [==============================] - 576s 200ms/step - loss: 1.4850 - accuracy: 0.5885 - val_loss: 1.6967 - val_accuracy: 0.5929\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    training_ds,\n",
    "    validation_data = validation_ds,\n",
    "    epochs = 25,\n",
    "    verbose = 1,\n",
    "    shuffle = False, ## is handled by dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b2f80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/pretrain_small_augs_25epo/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/pretrain_small_augs_25epo/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('saved_model/pretrain_small_augs_25epo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df65a73",
   "metadata": {},
   "source": [
    "# Load pretrained model and fine tune\n",
    "Repeat the process above with the desired words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d324dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_dict = get_meta(\n",
    "    data_path = os.path.join(os.path.expanduser(\"~\"),'coding_data/keyword_detection_nano/dataset/'),\n",
    "    wanted_words = ['licht', 'aus', 'party'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75154990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-16 18:34:32.542471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-16 18:34:32.579357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-16 18:34:32.579565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-16 18:34:32.580144: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-16 18:34:32.580716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-16 18:34:32.580879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-16 18:34:32.581024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-16 18:34:32.921254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-16 18:34:32.921447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-16 18:34:32.921601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-16 18:34:32.921732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6282 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:26:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "training_fns, validation_fns, background_fns = get_fns(meta_dict['training']['data_path'],\n",
    "                                                       meta_dict['training']['wanted_words'])\n",
    "\n",
    "training_ds = KeywordDataset(\n",
    "    training_fns,\n",
    "    background_fns,\n",
    "    meta_dict,\n",
    "    meta_dict['training']['batch_size'],\n",
    "    is_validation = False\n",
    ")\n",
    "validation_ds = KeywordDataset(\n",
    "    validation_fns,\n",
    "    background_fns,\n",
    "    meta_dict,\n",
    "    meta_dict['training']['batch_size'] * 2, ## a bigger batch size is possible since no gradients are used\n",
    "    is_validation = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc3d5b9",
   "metadata": {},
   "source": [
    "`get_model` loads the pretrained model, removes the last layer and adds in a new Dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e22c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1960)]            0         \n",
      "                                                                 \n",
      " reshape_2 (Reshape)         (None, 1, 49, 40, 1)      0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 1, 49, 40, 16)     256       \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 1, 49, 40, 16)     0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 1, 25, 20, 8)      10248     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 1, 25, 20, 8)      0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 4000)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 20005     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,509\n",
      "Trainable params: 20,005\n",
      "Non-trainable params: 10,504\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_labels = len(training_ds.vocab)\n",
    "fine_tune_model = get_model(n_labels, \n",
    "                            meta_dict, \n",
    "                            arch = 'small_conv', \n",
    "                            dropout = 0.5, \n",
    "                            pretrain_path = 'saved_model/pretrain_small_augs_25epo'\n",
    "                           )\n",
    "fine_tune_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a70bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate = meta_dict['training']['learning_rate']),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6738891c",
   "metadata": {},
   "source": [
    "## Fine tune classification layer only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c600275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "33/33 [==============================] - 7s 212ms/step - loss: 1.5167 - accuracy: 0.4309 - val_loss: 1.3205 - val_accuracy: 0.7372\n",
      "Epoch 2/5\n",
      "33/33 [==============================] - 7s 205ms/step - loss: 1.3413 - accuracy: 0.6135 - val_loss: 1.1172 - val_accuracy: 0.8029\n",
      "Epoch 3/5\n",
      "33/33 [==============================] - 7s 199ms/step - loss: 1.2157 - accuracy: 0.6715 - val_loss: 0.9715 - val_accuracy: 0.8175\n",
      "Epoch 4/5\n",
      "33/33 [==============================] - 7s 208ms/step - loss: 1.1399 - accuracy: 0.6773 - val_loss: 0.8504 - val_accuracy: 0.8540\n",
      "Epoch 5/5\n",
      "33/33 [==============================] - 7s 207ms/step - loss: 1.0273 - accuracy: 0.7314 - val_loss: 0.7653 - val_accuracy: 0.8662\n"
     ]
    }
   ],
   "source": [
    "fine_tune_history_frozen = fine_tune_model.fit(\n",
    "    training_ds,\n",
    "    validation_data = validation_ds,\n",
    "    epochs = 5,\n",
    "    verbose = 1,\n",
    "    shuffle = False, ## is handled by dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f0cc63",
   "metadata": {},
   "source": [
    "## Fine tune all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c63a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "33/33 [==============================] - 7s 203ms/step - loss: 0.9944 - accuracy: 0.7246 - val_loss: 0.7001 - val_accuracy: 0.8783\n",
      "Epoch 2/20\n",
      "33/33 [==============================] - 7s 202ms/step - loss: 0.9277 - accuracy: 0.7411 - val_loss: 0.6423 - val_accuracy: 0.8905\n",
      "Epoch 3/20\n",
      "33/33 [==============================] - 6s 193ms/step - loss: 0.9148 - accuracy: 0.7140 - val_loss: 0.6005 - val_accuracy: 0.8832\n",
      "Epoch 4/20\n",
      "33/33 [==============================] - 6s 189ms/step - loss: 0.8851 - accuracy: 0.7488 - val_loss: 0.5672 - val_accuracy: 0.8881\n",
      "Epoch 5/20\n",
      "33/33 [==============================] - 7s 199ms/step - loss: 0.8557 - accuracy: 0.7488 - val_loss: 0.5539 - val_accuracy: 0.8856\n",
      "Epoch 6/20\n",
      "33/33 [==============================] - 7s 198ms/step - loss: 0.7988 - accuracy: 0.7903 - val_loss: 0.5033 - val_accuracy: 0.8978\n",
      "Epoch 7/20\n",
      "33/33 [==============================] - 7s 200ms/step - loss: 0.8039 - accuracy: 0.7768 - val_loss: 0.4865 - val_accuracy: 0.8929\n",
      "Epoch 8/20\n",
      "33/33 [==============================] - 7s 199ms/step - loss: 0.7630 - accuracy: 0.7845 - val_loss: 0.4758 - val_accuracy: 0.8881\n",
      "Epoch 9/20\n",
      "33/33 [==============================] - 6s 189ms/step - loss: 0.7591 - accuracy: 0.7787 - val_loss: 0.4513 - val_accuracy: 0.8978\n",
      "Epoch 10/20\n",
      "33/33 [==============================] - 7s 207ms/step - loss: 0.7535 - accuracy: 0.7836 - val_loss: 0.4571 - val_accuracy: 0.8905\n",
      "Epoch 11/20\n",
      "33/33 [==============================] - 7s 199ms/step - loss: 0.7381 - accuracy: 0.7903 - val_loss: 0.4193 - val_accuracy: 0.8978\n",
      "Epoch 12/20\n",
      "33/33 [==============================] - 7s 203ms/step - loss: 0.7271 - accuracy: 0.8029 - val_loss: 0.4192 - val_accuracy: 0.8978\n",
      "Epoch 13/20\n",
      "33/33 [==============================] - 7s 199ms/step - loss: 0.7962 - accuracy: 0.7498 - val_loss: 0.4062 - val_accuracy: 0.8954\n",
      "Epoch 14/20\n",
      "33/33 [==============================] - 7s 202ms/step - loss: 0.7074 - accuracy: 0.7913 - val_loss: 0.4003 - val_accuracy: 0.8929\n",
      "Epoch 15/20\n",
      "33/33 [==============================] - 7s 200ms/step - loss: 0.7042 - accuracy: 0.7797 - val_loss: 0.3928 - val_accuracy: 0.8954\n",
      "Epoch 16/20\n",
      "33/33 [==============================] - 7s 200ms/step - loss: 0.6765 - accuracy: 0.7923 - val_loss: 0.3776 - val_accuracy: 0.8929\n",
      "Epoch 17/20\n",
      "33/33 [==============================] - 7s 198ms/step - loss: 0.6644 - accuracy: 0.8058 - val_loss: 0.3784 - val_accuracy: 0.8954\n",
      "Epoch 18/20\n",
      "33/33 [==============================] - 6s 193ms/step - loss: 0.6622 - accuracy: 0.8106 - val_loss: 0.3590 - val_accuracy: 0.9002\n",
      "Epoch 19/20\n",
      "33/33 [==============================] - 7s 201ms/step - loss: 0.6756 - accuracy: 0.8019 - val_loss: 0.3425 - val_accuracy: 0.9075\n",
      "Epoch 20/20\n",
      "33/33 [==============================] - 7s 200ms/step - loss: 0.6369 - accuracy: 0.8077 - val_loss: 0.3436 - val_accuracy: 0.9100\n"
     ]
    }
   ],
   "source": [
    "fine_tune_model.traiable = True\n",
    "fine_tune_history_thawn = fine_tune_model.fit(\n",
    "    training_ds,\n",
    "    validation_data = validation_ds,\n",
    "    epochs = 20,\n",
    "    verbose = 1,\n",
    "    shuffle = False, ## is handled by dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f402eaf1",
   "metadata": {},
   "source": [
    "# Export as Tensorflow Lite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af475e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_name = 'functional_fromPre_wAugs'\n",
    "export_dir = f'saved_model/{export_name}'\n",
    "#tf.saved_model.save(fine_tune_model, export_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0724b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "REP_DATA_SIZE = 100\n",
    "def representative_dataset_gen():\n",
    "    for i in range(REP_DATA_SIZE):\n",
    "            fn = random.choice(validation_ds.items)\n",
    "            label = validation_ds.get_label(fn)\n",
    "            audio = validation_ds.get_audio(fn, label)\n",
    "            spectro = validation_ds.get_spectrogram(audio).reshape(1,1960)\n",
    "            \n",
    "            yield [spectro]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94078a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-16 18:35:12.873350: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-08-16 18:35:12.873376: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-08-16 18:35:12.873955: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: saved_model/functional_fromPre_wAugs\n",
      "2022-08-16 18:35:12.875582: I tensorflow/cc/saved_model/reader.cc:81] Reading meta graph with tags { serve }\n",
      "2022-08-16 18:35:12.875595: I tensorflow/cc/saved_model/reader.cc:122] Reading SavedModel debug info (if present) from: saved_model/functional_fromPre_wAugs\n",
      "2022-08-16 18:35:12.879805: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2022-08-16 18:35:12.880740: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-08-16 18:35:12.927856: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: saved_model/functional_fromPre_wAugs\n",
      "2022-08-16 18:35:12.940495: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 66542 microseconds.\n",
      "2022-08-16 18:35:12.956452: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: 9, output_inference_type: 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized modelsize: 35136\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.inference_input_type = tf.compat.v1.lite.constants.INT8  #tf.int8#\n",
    "converter.inference_output_type = tf.compat.v1.lite.constants.INT8 #tf.int8#\n",
    "\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "tflite_model = converter.convert()\n",
    "tflite_model_size = open(f\"models/{export_name}_v1int8.tflite\", \"wb\").write(tflite_model)\n",
    "print(f\"Quantized modelsize: {tflite_model_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7499062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!xxd -i models/{export_name}.tflite > models/{export_name}.cc\n",
    "#REPLACE_TEXT = MODEL_TFLITE.replace('/', '_').replace('.', '_')\n",
    "#!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {MODEL_TFLITE_MICRO}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb42f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!code models/{export_name}.cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b31e96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
